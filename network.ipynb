{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from array import array\n",
    "import struct, os, random, math, pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#going to make a cusom Dataset class for my own MNIST files, even though pytorch already has MNIST\n",
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, data_path, train=True):\n",
    "        self.train = train\n",
    "        \n",
    "        if self.train:\n",
    "            images_filepath = os.path.join(data_path, 'train-images')\n",
    "            labels_filepath = os.path.join(data_path, 'train-labels')\n",
    "        else:\n",
    "            images_filepath = os.path.join(data_path, 'test-images')\n",
    "            labels_filepath = os.path.join(data_path, 'test-labels')\n",
    "        \n",
    "        self.images, self.labels = self.load_and_preprocess_data(images_filepath, labels_filepath)\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "\n",
    "    def load_and_preprocess_data(self, images_filepath, labels_filepath):\n",
    "        # Read labels\n",
    "        with open(labels_filepath, 'rb') as file:\n",
    "            magic, size = struct.unpack(\">II\", file.read(8))\n",
    "            if magic != 2049:\n",
    "                raise ValueError(f'Magic number mismatch, expected 2049, got {magic}')\n",
    "            labels = np.frombuffer(file.read(), dtype=np.uint8).astype(np.int64).copy()\n",
    "        \n",
    "        # Read images\n",
    "        with open(images_filepath, 'rb') as file:\n",
    "            magic, size, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
    "            if magic != 2051:\n",
    "                raise ValueError(f'Magic number mismatch, expected 2051, got {magic}')\n",
    "            images = np.frombuffer(file.read(), dtype=np.uint8).reshape(-1, rows, cols).copy()\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        images = torch.from_numpy(images).float().unsqueeze(1) / 255.0 \n",
    "        labels = torch.from_numpy(labels)\n",
    "        \n",
    "        return images, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        image = self.transform(image)\n",
    "        label = self.labels[idx]\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, loss_fn, optimizer, device):\n",
    "\n",
    "    #one run of this function goes through all images in the training data\n",
    "    #this function goes through a 64-size batch of the training data,\n",
    "    #computes the loss, and updates the model's weights and biases for that batch.\n",
    "    #it does this 938 times as that is what it takes to make sure all images\n",
    "    #are used for training. i.e 60000/64 = 938\n",
    "\n",
    "    # lets the model know we are about to train it\n",
    "    model.train()    \n",
    "    # tdqm just visually displays progress\n",
    "    #\"loader\" will give us 938 batches of 64 images\n",
    "    for images, labels in tqdm(loader, desc=\"Training\"):\n",
    "        #images is actually a list of images, same as labels\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        #need this since pytorch accumulates gradients by default\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #forward pass, image is passed through model to get prediction\n",
    "        # this is actually a list of multiple outputs, one for each image\n",
    "        outputs = model(images)\n",
    "\n",
    "        #calculate loss with our forward pass output and labels\n",
    "        # this is the loss averaged over all outputs and labels\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        #backproagation - computes gradients of loss with respect to model weights and biases\n",
    "        loss.backward()\n",
    "\n",
    "        #update weights and biases based on computed gradients\n",
    "        optimizer.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate(model, testing_loader, device):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval() \n",
    "\n",
    "    total_correct = 0 \n",
    "    total_samples = 0  \n",
    "    \n",
    "    # when evaluating, we dont want gradients to change\n",
    "    with torch.no_grad():  \n",
    "        # Iterate over batches from the data loader (this should just be one if we have the memory for it)\n",
    "        for images, labels in testing_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            # Perform a forward pass through the model\n",
    "            outputs = model(images)  # Pass the batch of images through the model to get predictions\n",
    "\n",
    "            # Get the predicted class indices\n",
    "            _, predicted = torch.max(outputs, 1)  \n",
    "\n",
    "            # Count the number of correct predictions in the current batch\n",
    "            total_correct += (predicted == labels).sum().item()  \n",
    "\n",
    "            # Count the total number of samples in the current batch\n",
    "            total_samples += labels.size(0)  # Add the number of samples in the current batch to the total count\n",
    "\n",
    "    # Compute accuracy\n",
    "    accuracy = total_correct / total_samples  # Calculate the accuracy as the ratio of correct predictions to total samples\n",
    "\n",
    "    return accuracy  # Return the computed accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_for_n_epochs(n, model, train_loader, test_loader, loss_fn, optimizer, device):\n",
    "    for epoch in range(n):\n",
    "        train_epoch(model, train_loader, loss_fn, optimizer, device)\n",
    "        test_accuracy = evaluate(model, test_loader, device)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "        print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "        print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PATH = './data'\n",
    "INPUT_LAYER_SIZE = 28*28\n",
    "HIDDEN_LAYER_SIZE = 16\n",
    "OUTPUT_LAYER_SIZE = 10\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mlp():\n",
    "    #check if a saved verson of the model exists\n",
    "    model = None\n",
    "    if os.path.exists(\"saved/model.pth\"):\n",
    "        model = torch.load(\"saved/model.pth\")\n",
    "    else:\n",
    "        nn.c\n",
    "        model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(INPUT_LAYER_SIZE, HIDDEN_LAYER_SIZE),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(HIDDEN_LAYER_SIZE, HIDDEN_LAYER_SIZE),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(HIDDEN_LAYER_SIZE, OUTPUT_LAYER_SIZE),\n",
    "        )\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = MNISTDataset(INPUT_PATH, train=True)\n",
    "    test_dataset = MNISTDataset(INPUT_PATH, train=False)\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    #train the model\n",
    "    train_for_n_epochs(EPOCHS, model, train_loader, test_loader, loss_fn, optimizer)\n",
    "\n",
    "    print(\"Training completed! Saving...\")\n",
    "    model_path = \"saved/model.pth\"\n",
    "    torch.save(model, model_path)\n",
    "    print(\"Model saved to \", model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cnn():\n",
    "    model = None\n",
    "    \n",
    "    # Check if a saved version of the model exists\n",
    "    if os.path.exists(\"saved/cnn_model.pth\"):\n",
    "        model = torch.load(\"saved/cnn_model.pth\")\n",
    "    else:\n",
    "        # Define a CNN model\n",
    "        model = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),  # Convolutional layer 1\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Pooling layer 1\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  # Convolutional layer 2\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Pooling layer 2\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),  # Convolutional layer 3\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Pooling layer 3\n",
    "\n",
    "            nn.Flatten(),  # Flatten the tensor to feed into fully connected layers\n",
    "            nn.Linear(128 * 3 * 3, 256),  # Fully connected layer 1\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 10)  # Fully connected layer 2 (output layer)\n",
    "        )\n",
    "\n",
    "    model = model.to(device)\n",
    "    # Create datasets\n",
    "    train_dataset = MNISTDataset(INPUT_PATH, train=True)\n",
    "    test_dataset = MNISTDataset(INPUT_PATH, train=False)\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    #train the model\n",
    "    train_for_n_epochs(EPOCHS, model, train_loader, test_loader, loss_fn, optimizer, device)\n",
    "\n",
    "    print(\"Training completed! Saving...\")\n",
    "    model_path = \"saved/cnn_model.pth\"\n",
    "    torch.save(model, model_path)\n",
    "    print(\"Model saved to \", model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bas\\AppData\\Local\\Temp\\ipykernel_21860\\3963810445.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(\"saved/cnn_model.pth\")\n",
      "Training: 100%|██████████| 469/469 [00:04<00:00, 101.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "Test Accuracy: 0.9818\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [00:04<00:00, 101.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3\n",
      "Test Accuracy: 0.9779\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [00:04<00:00, 101.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3\n",
      "Test Accuracy: 0.9837\n",
      "--------------------\n",
      "Training completed! Saving...\n",
      "Model saved to  saved/cnn_model.pth\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_cnn()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
